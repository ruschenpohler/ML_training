---
title: "A Conceptual Introduction to ML"
subtitle: "Internal Presentation, CEGA"
author: "Julius Ruschenpohler"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: inline
---
class: inverse, middle

```{R, setup, include = F}
# devtools::install_github("dill/emoGG")
library(pacman)
p_load(
  broom, tidyverse,
  ggplot2, ggthemes, ggforce, ggridges,
  latex2exp, viridis, extrafont, gridExtra,
  kableExtra, snakecase, janitor,
  data.table, dplyr, estimatr,
  lubridate, knitr, parallel,
  lfe, here, magrittr
)
# Define pink color
red_pink <- "#e64173"
turquoise <- "#20B2AA"
orange <- "#FFA500"
red <- "#fb6107"
blue <- "#3b3b9a"
green <- "#8bb174"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
purple <- "#6A5ACD"
slate <- "#314f4f"
# Dark slate grey: #314f4f
# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  warning = F,
  message = F
)
opts_chunk$set(dev = "svg")
options(device = function(file, width, height) {
  svg(tempfile(), width = width, height = height)
})
options(crayon.enabled = F)
options(knitr.table.format = "html")
# A blank theme for ggplot
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -0.5, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_simple <- theme_bw() + theme(
  line = element_blank(),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text.x = element_text(size = 18, family = "STIXGeneral"),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  # plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_math <- theme_void() + theme(
  text = element_text(family = "MathJax_Math"),
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_serif <- theme_void() + theme(
  text = element_text(family = "MathJax_Main"),
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes <- theme_void() + theme(
  text = element_text(family = "Fira Sans Book"),
  axis.title = element_text(size = 18),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = grey_light,
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_set(theme_gray(base_size = 20))
# Column names for regression results
reg_columns <- c("Term", "Est.", "S.E.", "t stat.", "p-Value")
# Function for formatting p values
format_pvi <- function(pv) {
  return(ifelse(
    pv < 0.0001,
    "<0.0001",
    round(pv, 4) %>% format(scientific = F)
  ))
}
format_pv <- function(pvs) lapply(X = pvs, FUN = format_pvi) %>% unlist()
# Tidy regression results table
tidy_table <- function(x, terms, highlight_row = 1, highlight_color = "black", highlight_bold = T, digits = c(NA, 3, 3, 2, 5), title = NULL) {
  x %>%
    tidy() %>%
    select(1:5) %>%
    mutate(
      term = terms,
      p.value = p.value %>% format_pv()
    ) %>%
    kable(
      col.names = reg_columns,
      escape = F,
      digits = digits,
      caption = title
    ) %>%
    kable_styling(font_size = 20) %>%
    row_spec(1:nrow(tidy(x)), background = "white") %>%
    row_spec(highlight_row, bold = highlight_bold, color = highlight_color)
}
```


.pull-left[
.smaller[

### FIRST PART
I. Standard Regression Inference  
II. Why Machine learning?  
III. What is the Intuition behind ML?
 
]
]

.pull-right[
.smaller[
  
### SECOND PART
IV. Where Can ML be Put to Use?  
V. What Caveats Apply?  
VI. What Are Current Areas of Development?

]
]

---

layout: false
# FIRST PART

## Machine Learning: Why and How?


.pull-left[

### I. [Standard Regression Inference](#standard)
  - [An Inference Problem](#standard)
  - [Link Function](#standard1)
  - [Loss Function](#standard2)
  - [Minimizing Statistical Loss](#standard3)
  
]

.pull-right[

### II. [Why Machine Learning](#whyMLA)
  - [High Dimensionality](#whyMLA)
  - [Prediction vs. Causal Inference](#whyMLB)
  - [Bias-variance Trade-off](#whyMLC)
  
### III. [What is the Intuition behind ML?](#whatMLA)
  - [Goal of Machine Learning](#whatMLA)
  - [Supervised vs. Unsupervised Learning](#whatMLB)
  - [Regularization](#whatMLC)
  - [Classification](#whatMLD)

]


---
name: standard

# I. Standard Regression Inference

## An Inference Problem

Given a limited set of predictors, we would like to predict an outcome: $\mathbb {E} \left[ y_{i} | x_{i} \right]$.

That is, what is $y_{i}$ given $x_{i}$?

- Example: What impact does red meat $x_{i}$ have on general health $y_{i}$?

### Practically, we have to:

.pull-left[

1. Link predictors to outcome (link fct)
1. Define best fit criteria (loss fct)
1. Maximize fit
]

.pull-right[

**Can we make <br/> sense of this <br/> graphically?**
]

---
name: standardA_1

# I. Standard Regression Inference

## 1. Link Function

.pull-left[

Data on daily red meat consumption

$$X = \begin{bmatrix} 
    x_{1,1} & x_{1,2} & \dots \\
    \vdots & \ddots & \\
    x_{n,1} &        & x_{n,p} 
    \end{bmatrix}$$
]

.pull-right[

Data on daily measures of blood pressure

$$Y = \begin{bmatrix} 
    y_{1,1} & y_{1,2} & \dots \\
    \vdots & \ddots & \\
    y_{n,1} &        & y_{n,k} 
    \end{bmatrix}$$
]

**Link fct**: What is the relationship that links meat consumption and blood pressure?

```{r, echo=FALSE}
library(png)
library(grid)
fctForms <- readPNG("pics/fctForms.png")
grid.raster(fctForms, x = unit(0.5, "npc"), y = unit(0.75, "npc"),width = 0.84, height = 0.4,)
```


---
name: standard1

# I. Standard Regression Inference

In other words, we need a function that links meat consumption to blood pressure:

$$\mathbb { E } \left[ y_{i} | x_{i} \right] = f \left( \eta _ { i } \right)$$

Here, $\eta_{i}$ is our model that predicts blood pressure.

For simplicity, let blood pressure be a linear fct of meat consumption:

.pull-left[

<br/>
<br/>

$\eta _ { i } = \alpha + x_{i} \beta$

<br/>
<br/>

But we still don't know **_what_ straight line best approximates the relationship**!
]

.pull-right[

```{r, echo=FALSE}
library(png)
library(grid)
linFctForm <- readPNG("pics/linFctForm.png")
grid.raster(linFctForm, x = unit(0.5, "npc"), y = unit(0.55, "npc"),width = 0.75, height = 1,)
```
]


---
name: standard2

# I. Standard Regression Inference

## 2. Loss Function

Second, we need to define "best fit".

Equivalent to saying we need to minimize statistical loss $\ell ( \alpha , \beta )$.

Can we make sense of this graphically?

```{r, fig.width=6, fig.height=3.6, echo=FALSE}
library(png)
library(grid)
leastSquares <- readPNG("pics/leastSquares.png")
grid.raster(leastSquares, x = unit(0.5, "npc"), y = unit(0.5, "npc"), width = 1, height = 1,)
```



---
name: standard2a

# I. Standard Regression Inference

With quadratic loss fct $\ell(\alpha,\beta)$ (as per "ordinary least squares"), we have:

$$\ell(\alpha,\beta) = \sum_{i=1} ^ {n} \left( y_{i} - \eta_{i} \right)^{2}$$
That is, we consider the squared distance between our model prediction $\eta_{i}$ of blood pressure and real measures of blood pressure $y_{i}$.

This works great!

<br/>

But what if we are not only interested in the effect of meat consumption?

What if the **set of predictors $p$ is large**?

**Potentially even: $p>n$**!


---
name: whyMLB

# II. Why Machine Learning?

## A) High Dimensionality and Sparcity







---
name: whyMLC

# II. Why Machine Learning?

## B) Prediction =/= Causal Inference
Collinearity


---
name: whyMLD

# II. Why Machine Learning?

## D) Bias-variance Trade-off


---
name: whatMLA

# III. What is the Intuition behind ML?

## A) Goal of Machine Learning

Construct a sparse model $\eta$ from high-dimensional data

Model $\eta_{i}$ predicts label $Y_{i}$ (=outcome) from a subset of features $X_{i}$ (=predictors)

Features $X_{i}$ need to be selected (or weighed) such that model $\eta$ best generalizes to new data


---
name: whatMLB

# III. What is the Intuition behind ML?

## B) Supervised vs. Unsupervised ML

### Supervised Machine Learning

Predicting a label (~outcome)
+ Minimizing some kind of prediction error

### Unsupervised Machine Learning

Classify data 
+ Minimizing misclassification


- This presentation focusses on supervised ML.


---
name: whatMLC

# III. What is the Intuition behind ML?





---
name: whatMLD

# III. What is the Intuition behind ML?

## D) Penalized Regression

Open question: **How to deal with high dimensionality in data?**

Let's remember the loss fct: $\ell(\alpha,\beta)$

- However, now the set of potential predictors is too large!

To make the model less complex, we would like to select only the most important predictors.

Let's penalize model complexity:

$$\min \left\{\ell(\alpha,\beta) + n \lambda \sum_{j=1}^{p} K_{j} \left (\beta_{j} \right) \right\}$$

**Shrinkage**: $\lambda$ governs the magnitude of penalty or shrinkage.

**Regularization**: $K_{j} \left( \beta_{j} \right)$ is a cost fct that defines the penalty regime


---
name: whatMLD1

# III. What is the Intuition behind ML?

## D) Penalized Regression

### Types of cost fcts

```{r, fig.width=9, fig.height=2.5, echo=FALSE}
library(png)
library(grid)
costFcts <- readPNG("pics/costFcts.png")
grid.raster(costFcts, x = unit(0.5, "npc"), y = unit(0.5, "npc"), width = 1, height = 1,)
```

Take-away: Different ways of penalizing model complexity.

But **__how much__ do we want to penalize complexity**? 

That is, how to choose $\lambda$? In other words, how do we validate a chosen $\lambda$?




---
name: whatMLD2

# III. What is the Intuition behind ML?

### 1. Training and Test Sample

.pull-left[

$$X_{train} = \begin{bmatrix} 
    x_{1,1} & x_{1,2} & \dots \\
    \vdots & \ddots & \\
    x_{n,1} &        & x_{n,p} 
    \end{bmatrix}$$
    
$$X_{test} = \begin{bmatrix} 
    x_{1,1} & x_{1,2} & \dots \\
    \vdots & \ddots & \\
    x_{n,1} &        & x_{n,p} 
    \end{bmatrix}$$
]


.pull-right[

$$Y_{train} = \begin{bmatrix} 
    y_{1,1} & y_{1,2} & \dots \\
    \vdots & \ddots & \\
    y_{n,1} &        & y_{n,k} 
    \end{bmatrix}$$
    
$$?_{test} = \begin{bmatrix} 
    ?_{1,1} & ?_{1,2} & \dots \\
    \vdots & \ddots & \\
    ?_{n,1} &        & ?_{n,k} 
    \end{bmatrix}$$
]

### 2. K-fold cross-validation

Split the sample in one training and **many test samples**

### 3. Leave-one-out cross-validation

Split sample in **as many sub-samples as there are observations**


---
name: whatMLD2

# III. What is the Intuition behind ML?

## Take-away

<br/>

In **high-dimensional environments**, we need to find ways to come up with **sparse models**.

One way is to **penalize model complexity** as a part of the loss fct $\ell(\alpha,\beta)$.

That is, we incorporate a **shrinkage** parameter $\lambda$ and a **cost fct** $K_{j} \left (\beta_{j} \right)$.

**Regularization** is the process of shrinking and selecting some parameters, but not others.

**Cross-validation** techniques are a way to choose the optimal value for $\lambda$.

That is, a value of $\lambda$ such that our model **best predicts new data**.

We call this data the **test sample**.



---
name: whatMLE

# III. What is the Intuition behind ML?

## E) Classification/Non-parametric Methods

SCRAP?


---

layout: false
# SECOND PART

## Application, Caveats, and Developments in ML


.pull-left[

### IV. [Where to Use ML?](#whereML1)
  - [Prediction and Measurement](#whereML1)
  - [Targeting](#whereML2)
  - [Heterogeneity](#whereML3)
  - [Pre-analysis Plans](#whereML4)
  
### V. [What Caveats Apply](#caveats1)
  - [Ethics](#caveats1)
  - [Opacity](#caveats2)
  - [Model Complexity](#caveats3)
  - [Reproducibility](#caveats4)
  - [Model Robustness](#caveats5)
  - [Causality](#caveats6)
  
]

.pull-right[
  
### VI. [Current Areas of Development](#develop1)
  - [Prediction](#develop1)
  - [Model Robustness](#develop2)
  - [Causality and heterogeneity](#develop3)

]



---

name: whereML1

# IV. Where can ML be Put to Use?

## A) Prediction and Measurement

### Predicting Firm Performance (McKenzie & Sansone, JDE 2019)

Paper: "Predicting Entrepreneurial Success is Hard: Evidence from a Business Plan Competition in Nigeria"

* YouWIN! **business plan competition** in Nigeria
* Data from 1,000 winners and 1,000 losers, baseline and 3-yrs follow-up

3 methods to predict entrepreneurial success:
1. **Human judges** score business plans
1. **Simple logistic regression models** by expert academics
1. **Machine learning methods**, i.e. LASSO, Support vector machines (SVM), and Boosting


---

name: whereML1a

# IV. Where can ML be Put to Use?

## A) Prediction and Measurement

```{r, fig.width=7.5, fig.height=5, echo=FALSE}
library(png)
library(grid)
McK_S <- readPNG("pics/McK&S2019_main.png")
grid.raster(McK_S, x = unit(0.5, "npc"), y = unit(0.5, "npc"), width = 1, height = 1,)
```

---

name: whereML1b

# IV. Where can ML be Put to Use?

### Predicting Poverty (Blumenstock et al., Science 2015)

Paper: "Predicting Poverty and Wealth from Mobile Phone Metadata"

+ Also helpful: Blumenstock (AEAPP, 2018) "Estimating Economic Characteristics with Phone Data"

* Rwandan **mobile phone metadata** over a year (N=1,5M, calls, texts, mobility)
* Follow-up **phone surveys** of geographically stratified random sample (n=856) on assets, housing, welfare
* Authors ..
  + .. use training sample to train and cross-validate a set of algorithms
  + .. **predict wealth for test sample** (n~1.5M)


---

name: whereML1c

# IV. Where can ML be Put to Use?

```{r, fig.width=7.5, fig.height=5, echo=FALSE}
library(png)
library(grid)
B_distrib <- readPNG("pics/B2015_distrib.png")
grid.raster(B_distrib, x = unit(0.5, "npc"), y = unit(0.5, "npc"), width = 1, height = 1,)
```

---

name: whereML1d

# IV. Where can ML be Put to Use?

```{r, fig.width=9, fig.height=3, echo=FALSE}
library(png)
library(grid)
B_val <- readPNG("pics/B2015_val.png")
grid.raster(B_val, x = unit(0.5, "npc"), y = unit(0.5, "npc"), width = 1, height = 1,)
```

.pull-left[
  
**A**: Predicted wealth composite score
]

.pull-right[

**B**: Actual wealth (Admin 2010, N=12,792 HHs)
]


**Prediction accuracy for asset ownership was 64 to 92 percent**!

---

name: whereML2

# IV. Where can ML be Put to Use?

## B) Targeting


---

name: whereML3

# IV. Where can ML be Put to Use?

## C) Causality

### Covariate Selection through Regularization (Belloni et al., ReStud 2014)

Paper: “Inference on Treatment Effects after Selection among High-Dimensional Controls”

Scenario: Experimental study, high-dimensional data ($p_{large}>n$)
Goal: Select set of predictors $p_{small}<n$

“Double-selection procedure”:
1. Use LASSO to select set of predictors for outcome $y_{i}$
2. Use LASSO to select set of predictors for treatment assignment $d_{i}$
3. Choose predictors in union of sets as control variables


---

name: whereML3a

# IV. Where can ML be Put to Use?

### Boosting Relevance of Instrumental Variables


---

name: whereML4

# IV. Where can ML be Put to Use?

## D) Hetereogeneity

### Optimal Selection of Heterogeneous Variables


---

name: whereML5

# IV. Where can ML be Put to Use?

## E) Pre-analysis Plans

### Decision Heuristics Based on ML


---

name: caveats1

# V. What Caveats Apply?

## A) Ethics

###


---

name: caveats2

# V. What Caveats Apply?

## B) Opacity or explainability ("black box")

```{r, echo=FALSE}
library(png)
library(grid)
blackbox <- readPNG("pics/Sci_blackbox.png")
grid.raster(blackbox, x = unit(0.4, "npc"), y = unit(0.6, "npc"),width = 0.75, height = 0.7,)
```

[Hutson, 2018a] (https://www.sciencemag.org/news/2018/05/ai-researchers-allege-machine-learning-alchemy)


---

name: caveats2a

# V. What Caveats Apply?

### Two sides to this argument
   
1. **Opaque process**
+ De-bugging
    
2. **Prediction without regard of DGP**
+ Intelligibility
+ Trust
+ External validity
    
    
- Literature: see, e.g., Ribeiro et al., 2016


---

name: caveats3

# V. What Caveats Apply?

## C) Model Complexity

### Many researcher degrees of freedom

* Data pre-processing
  + Example: Text analysis (libraries, tokenization, n-grams, etc.)

* Shrinkage and Regularization

* Cross-validation



---

name: caveats4

# V. What Caveats Apply?

## D) Reproducibility

```{r, echo=FALSE}
library(png)
library(grid)
repro <- readPNG("pics/Sci_Reproducibility.png")
grid.raster(repro, x = unit(0.4, "npc"), y = unit(0.6, "npc"),width = 0.7, height = 0.45,)
```

    Hutson, 2018b (https://science.sciencemag.org/content/359/6377/725)

    
---

name: caveats4a

# V. What Caveats Apply?

### Encoding causal assumptions
    
* Testability detection via graphical representations (e.g., DAGs a la Pearl, 1995)
    
- LIterature: see, e.g. Dacrema et al., 2019; Dodge et al., 2019; McDermott et al., 2019; Mitchell et al., 2019.


---

name: caveats5

# V. What Caveats Apply?

## E) Model Robustness or Adaptability

### Problem
    
External validity and out-of-sample prediction
1. Performance under changed environmental conditions]
2. Adversarial attacks

### Solutions
    
* Life-long maschine learning (Chen and Liu, 2016)
* Transfer learning
* Domain adaptation


---

name: caveats6

# V. What Caveats Apply

## F) Causality

### Selection-on-observables assumption 
with optimized set of control variables

### Selective Labels for the Tested



---

name: develop1

# VI. Current Areas of Development

## A) Re: Prediction


---

name: develop2

# VI. Current Areas of Development

## A) Re: Model Robustness


---

name: develop3

# VI. Current Areas of Development

## B) Re: Causality and Heterogeneous Treatment Effects




---

name: conclusion

# Conclusion


---

name: extras1

# Additional Slides

## Minimizing Statistical Loss

Minimize empirical risk over space of functions (e.g., linear function, quadratic loss, neural networks, etc.):

\begin{equation}
f ^ {*} = \arg \min_{f \in \mathcal {H}} \frac {1} {n} \sum L \left( f \left( x_{i} \right) , y_{i} \right)
\label{minloss}
\end{equation}

Practical steps:
1. Choose type of loss function, e.g. quadratic $\left( f \left( x_{i} \right) - y_{i} \right) ^ {2}$
1. Choose starting point
1. Use Stochastic Gradient Descent to find minimum in iterative process

```{r, echo=FALSE}
library(png)
library(grid)
SGD_2 <- readPNG("pics/SGD_2.png")
grid.raster(SGD_2, x = unit(0.5, "npc"), y = unit(0.75, "npc"),width = 0.5, height = 0.5,)
```
