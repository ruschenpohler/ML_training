---
title: "A Conceptual Introduction to ML"
subtitle: "Internal Presentation, CEGA"
author: "Julius Ruschenpohler"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: inline
---
class: inverse, middle

```{R, setup, include = F}
# devtools::install_github("dill/emoGG")
library(pacman)
p_load(
  broom, tidyverse,
  ggplot2, ggthemes, ggforce, ggridges,
  latex2exp, viridis, extrafont, gridExtra,
  kableExtra, snakecase, janitor,
  data.table, dplyr, estimatr,
  lubridate, knitr, parallel,
  lfe, here, magrittr
)
# Define pink color
red_pink <- "#e64173"
turquoise <- "#20B2AA"
orange <- "#FFA500"
red <- "#fb6107"
blue <- "#3b3b9a"
green <- "#8bb174"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
purple <- "#6A5ACD"
slate <- "#314f4f"
# Dark slate grey: #314f4f
# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  warning = F,
  message = F
)
opts_chunk$set(dev = "svg")
options(device = function(file, width, height) {
  svg(tempfile(), width = width, height = height)
})
options(crayon.enabled = F)
options(knitr.table.format = "html")
# A blank theme for ggplot
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -0.5, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_simple <- theme_bw() + theme(
  line = element_blank(),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text.x = element_text(size = 18, family = "STIXGeneral"),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  # plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_math <- theme_void() + theme(
  text = element_text(family = "MathJax_Math"),
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_serif <- theme_void() + theme(
  text = element_text(family = "MathJax_Main"),
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes <- theme_void() + theme(
  text = element_text(family = "Fira Sans Book"),
  axis.title = element_text(size = 18),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = grey_light,
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_set(theme_gray(base_size = 20))
# Column names for regression results
reg_columns <- c("Term", "Est.", "S.E.", "t stat.", "p-Value")
# Function for formatting p values
format_pvi <- function(pv) {
  return(ifelse(
    pv < 0.0001,
    "<0.0001",
    round(pv, 4) %>% format(scientific = F)
  ))
}
format_pv <- function(pvs) lapply(X = pvs, FUN = format_pvi) %>% unlist()
# Tidy regression results table
tidy_table <- function(x, terms, highlight_row = 1, highlight_color = "black", highlight_bold = T, digits = c(NA, 3, 3, 2, 5), title = NULL) {
  x %>%
    tidy() %>%
    select(1:5) %>%
    mutate(
      term = terms,
      p.value = p.value %>% format_pv()
    ) %>%
    kable(
      col.names = reg_columns,
      escape = F,
      digits = digits,
      caption = title
    ) %>%
    kable_styling(font_size = 20) %>%
    row_spec(1:nrow(tidy(x)), background = "white") %>%
    row_spec(highlight_row, bold = highlight_bold, color = highlight_color)
}
```


.pull-left[
.smaller[

### FIRST PART
I. Standard Regression Inference
II. Why Machine learning?  
III. What is the Intuition behind ML?
 
]
]

.pull-right[
.smaller[
  
### SECOND PART
IV. Where Can ML be Put to Use?  
V. What Caveats Apply?

]
]

---

layout: false
# FIRST PART

## Machine Learning: Why and How?


.pull-left[

### I. [Standard Regression Inference](#standard)
  - [An Inference Problem](#standard)
  - [Link Function](#standard1)
  - [Loss Function](#standard2)
  - [Minimizing Statistical Loss](#standard3)
  
]

.pull-right[

### II. [Why Machine Learning](#whyMLA)
  - [High Dimensionality](#whyMLA)
  - [Prediction vs. Causal Inference](#whyMLB)
  - [Bias-variance Trade-off](#whyMLC)
  
### III. [What is the Intuition behind ML?](#whatMLA)
  - [Goal of Machine Learning](#whatMLA)
  - [Penalized Regression](#whatMLB)
  - [Supervised vs. Unsupervised Learning](#whatMLC)
  - [More Complex ML Algorithms](#whatMLD)
  - [Take-away](#whatMLE)

]


---
name: standard

# I. Standard Regression Inference

## An Inference Problem

Given a set of predictors, we would like to predict an outcome: $\mathbb {E} \left[ y_{i} | x_{i} \right]$.

That is, what is $y_{i}$ given $x_{i}$?

- Example: What impact does red meat $x_{i}$ have on general health $y_{i}$?

### Practically, we have to:

.pull-left[

1. Link predictors to outcome (link fct)
1. Define best fit criteria (loss fct)
1. Maximize fit
]

.pull-right[

**Can we make <br/> sense of this <br/> graphically?**
]

---
name: standardA_1

# I. Standard Regression Inference

## 1. Link Function

.pull-left[

Data on daily red meat consumption

$$X = \begin{bmatrix} 
    x_{1,1} & x_{1,2} & \dots \\
    \vdots & \ddots & \\
    x_{n,1} &        & x_{n,p} 
    \end{bmatrix}$$
]

.pull-right[

Data on daily measures of blood pressure

$$Y = \begin{bmatrix} 
    y_{1,1} & y_{1,2} & \dots \\
    \vdots & \ddots & \\
    y_{n,1} &        & y_{n,k} 
    \end{bmatrix}$$
]

**Link fct**: What is the relationship that links meat consumption and blood pressure?

```{r, echo=FALSE}
library(png)
library(grid)
fctForms <- readPNG("pics/fctForms.png")
grid.raster(fctForms, x = unit(0.5, "npc"), y = unit(0.75, "npc"),width = 0.84, height = 0.4,)
```


---
name: standard1

# I. Standard Regression Inference

In other words, we need a function that links meat consumption to blood pressure:

$$\mathbb { E } \left[ y_{i} | x_{i} \right] = f \left( \eta _ { i } \right)$$

Here, $\eta_{i}$ is our model that predicts blood pressure.

For simplicity, let blood pressure be a linear fct of meat consumption:

.pull-left[

<br/>
<br/>

$\eta _ { i } = \alpha + x_{i} \beta$

<br/>
<br/>

But we still don't know **_what_ straight line best approximates the relationship**!
]

.pull-right[

```{r, echo=FALSE}
library(png)
library(grid)
linFctForm <- readPNG("pics/linFctForm.png")
grid.raster(linFctForm, x = unit(0.5, "npc"), y = unit(0.55, "npc"),width = 0.75, height = 1,)
```
]


---
name: standard2

# I. Standard Regression Inference

## 2. Loss Function

Second, we need to define "best fit".

Equivalent to saying we need to minimize statistical loss $\ell ( \alpha , \beta )$.

Can we make sense of this graphically?

```{r, fig.width=6, fig.height=3.6, echo=FALSE}
library(png)
library(grid)
leastSquares <- readPNG("pics/leastSquares.png")
grid.raster(leastSquares, x = unit(0.5, "npc"), y = unit(0.5, "npc"), width = 1, height = 1,)
```



---
name: standard2a

# I. Standard Regression Inference

With quadratic loss fct $\ell(\alpha,\beta)$ (as per "ordinary least squares"), we have:

$$\ell(\alpha,\beta) = \sum_{i=1} ^ {n} \left( y_{i} - \eta_{i} \right)^{2}$$
That is, we consider the squared distance between our model prediction $\eta_{i}$ of blood pressure and real measures of blood pressure $y_{i}$.

Now, all we need is to find the **straight line that minimizes $\ell(\alpha,\beta)$**.

<br/>

Great, this works!


---
name: whyMLB

# II. Why Machine Learning?

## A) High Dimensionality and Sparcity


But what if we are not only interested in the effect of meat consumption?

**What if the set of predictors $p$ is large**?

Potentially even: $p>n$!

.pull-left[

$$X = \begin{bmatrix} 
    x_{1,1} & x_{1,2} & x_{1,3} &x_{1,4} & \dots \\
    \vdots & \ddots & \\
    x_{n<p,1} &        &         &    & x_{n<p,p_{large}} 
    \end{bmatrix}$$
]

.pull-right[

$$Y = \begin{bmatrix} 
    y_{1,1} & y_{1,2} & \dots \\
    \vdots & \ddots & \\
    y_{n,1} &          & y_{n,p} 
    \end{bmatrix}$$
]

<br/>

We need ways to generate **sparse models** that predict well in **high-dimensional space** (~with many potential covariates).


---
name: whyMLC

# II. Why Machine Learning?

## B) Prediction $\neq$ Causal Inference

### Causal Inference

**Conditional covariances** (true data generating process)
* Example: All else equal, does meat consumption $x_{i}$ predict blood pressure $y_{i}$?
* Search for $\hat{\beta}$

### Prediction

**Best prediction**
* Example: What is the best prediction for blood pressure $y_{i}$ given a large set of predictors $p_{large}$?
* Origin: Face recognition
* Search for $\hat{y}$

---
name: whatMLA

# III. What is the Intuition behind ML?

## A) Goal of Machine Learning

**Construct a sparse model $\eta$ from high-dimensional data**

Model $\eta_{i}$ predicts label $Y_{i}$ (=outcome) from a subset of features $X_{i}$ (= predictors)

Features $X_{i}$ need to be selected (or weighed) such that model $\eta$ best generalizes to new data ("out of sample")

But: At the core, **still (out-of-sample) loss minimization**!


---
name: whatMLB

# III. What is the Intuition behind ML?

## B) Penalized Regression or Regularization

Open question: **How to deal with high dimensionality in data?**

Let's remember the loss fct: $\ell(\alpha,\beta)$

* However, now the set of potential predictors is large!

To make the model less complex, we would like to select only the most important predictors ("regularization").

Let's penalize model complexity:

$$\min \left\{\ell(\alpha,\beta) + n \lambda \sum_{j=1}^{p} K_{j} \left (\beta_{j} \right) \right\}$$

**Cost fct**: $K_{j} \left( \beta_{j} \right)$ is a cost fct that defines the penalty regime

**Shrinkage**: $\lambda$ governs the magnitude of penalty or shrinkage.


---
name: whatMLB1

# III. What is the Intuition behind ML?

### Types of cost fcts

```{r, fig.width=9, fig.height=2.5, echo=FALSE}
library(png)
library(grid)
costFcts <- readPNG("pics/costFcts.png")
grid.raster(costFcts, x = unit(0.5, "npc"), y = unit(0.5, "npc"), width = 1, height = 1,)
```

Take-away: Different ways of penalizing model complexity.

<br/>

But **__how much__ do we want to penalize complexity**? 

That is, how to choose $\lambda$? In other words, how do we validate a chosen $\lambda$?


---
name: whatMLB2

# III. What is the Intuition behind ML?

### 1. Training and Test Sample

.pull-left[

$$X_{train} = \begin{bmatrix} 
    x_{1,1} & x_{1,2} & \dots \\
    \vdots & \ddots & \\
    x_{n,1} &        & x_{n,p} 
    \end{bmatrix}$$
    
$$X_{test} = \begin{bmatrix} 
    x_{1,1} & x_{1,2} & \dots \\
    \vdots & \ddots & \\
    x_{n,1} &        & x_{n,p} 
    \end{bmatrix}$$
]


.pull-right[

$$Y_{train} = \begin{bmatrix} 
    y_{1,1} & y_{1,2} & \dots \\
    \vdots & \ddots & \\
    y_{n,1} &        & y_{n,k} 
    \end{bmatrix}$$
    
$$?_{test} = \begin{bmatrix} 
    ?_{1,1} & ?_{1,2} & \dots \\
    \vdots & \ddots & \\
    ?_{n,1} &        & ?_{n,k} 
    \end{bmatrix}$$
]

### 2. $k$-fold cross-validation

Same principle as training-test sample split (details in [Additional Slides](#extras2))


### 3. Leave-one-out cross-validation

Split sample in **as many sub-samples as there are observations**


---
name: whyMLC

# II. Why Machine Learning?

## C) Bias-variance Trade-off



---
name: whatMLD

# III. What is the Intuition behind ML?

## D) Supervised vs. Unsupervised ML

### Supervised Machine Learning

Predict a label (~outcome)
* Minimize some type of prediction error

### Unsupervised Machine Learning

Classify data
* Minimizing misclassification


- This presentation focusses on supervised ML.


---
name: whatMLE

# III. What is the Intuition behind ML?


## E) More Complex ML Algorithms

* **Regression Trees**

  + Iterative splits
  + Pools observations into orthogonal sub-spaces over features
  + Based on similarity in the label
  + Criteria e.g. Gini

* **Random Forests**

  + Ensemble of regression trees that imposes randomness across trees

* **Gradient Boosted Trees**

  + Ensemble of regression trees that are iteratively fit to previous trees' residuals


---
name: whatMLE

# III. What is the Intuition behind ML?

## Take-away

<br/>

Central issue: In **high-dimensional environments**, we need to find ways to come up with **sparse models** that predict outcomes well **out of sample**.

* One way is to **penalize model complexity** as a part of the loss fct $\ell(\alpha,\beta)$.

* That is, we incorporate a **shrinkage** parameter $\lambda$ and a **cost fct** $K_{j} \left (\beta_{j} \right)$.

* **Regularization** is the process of shrinking and selecting some parameters, but not others.

* **Cross-validation** techniques are a way to set $\lambda$ such that our model $\eta$ best predicts out of sample.

* We call this data the **test sample**, and data the model is fitted on the **training data**.




---
name: whatMLE2

# III. What is the Intuition behind ML?

* **Neural Nets**

  + Inter-connected nodes organized into layers (~structure of the brain)
  + Signal passes through nodes which perform non-linear transformation on signal
  + Resulting prediction is highly flexible
  + Black box

```{r, fig.width=6.5, fig.height=3.5, echo=FALSE}
library(png)
library(grid)
neuralNet <- readPNG("pics/neuralNet.png")
grid.raster(neuralNet, x = unit(0.5, "npc"), y = unit(0.5, "npc"), width = 1, height = 1,)
```

---

layout: false
# SECOND PART

## Application, Caveats, and Developments in ML


.pull-left[

### IV. [Where to Use ML?](#whereML1)
  - [Prediction and Measurement](#whereML1)
  - [Targeting](#whereML2)
  - [Heterogeneity](#whereML3)
  - [Pre-analysis Plans](#whereML4)
]

.pull-right[

### V. [What Caveats Apply](#caveats1)
  - [Ethics](#caveats1)
  - [Opacity](#caveats2)
  - [Model Complexity](#caveats3)
  - [Reproducibility](#caveats4)
  - [Model Robustness](#caveats5)
  - [Causality](#caveats6)
]



---

name: whereML1

# IV. Where can ML be Put to Use?

## A) Prediction and Measurement

### Predicting Firm Performance (McKenzie & Sansone, JDE 2019)

Paper: "_Predicting Entrepreneurial Success is Hard: Evidence from a Business Plan Competition in Nigeria_"

* YouWIN! **business plan competition** in Nigeria
* Data from 1,000 winners and 1,000 losers, baseline and 3-yrs follow-up

3 methods to predict entrepreneurial success:
1. **Human judges** score business plans
1. **Simple logistic regression models** by expert academics
1. **Machine learning methods**, i.e. LASSO, Support vector machines (SVM), and Boosting


---

name: whereML1a

# IV. Where can ML be Put to Use?

## A) Prediction and Measurement

```{r, fig.width=7.5, fig.height=5, echo=FALSE}
library(png)
library(grid)
McK_S <- readPNG("pics/McK&S2019_main.png")
grid.raster(McK_S, x = unit(0.5, "npc"), y = unit(0.5, "npc"), width = 1, height = 1,)
```

---

name: whereML1b

# IV. Where can ML be Put to Use?

### Predicting Poverty (Blumenstock et al., Science 2015)

Paper: "_Predicting Poverty and Wealth from Mobile Phone Metadata_"

+ Also helpful: Blumenstock (AEAPP, 2018) "_Estimating Economic Characteristics with Phone Data_"

* Rwandan **mobile phone metadata** over a year (N = 1,5M, calls, texts, mobility)
* Follow-up **phone surveys** of geographically stratified random sample (n = 856) on assets, housing, welfare
* Authors ..
  + .. use training sample to train and cross-validate a set of algorithms
  + .. **predict wealth for test sample** (n $\approx$ 1.5M)


---

name: whereML1c

# IV. Where can ML be Put to Use?

```{r, fig.width=7.5, fig.height=5, echo=FALSE}
library(png)
library(grid)
B_distrib <- readPNG("pics/B2015_distrib.png")
grid.raster(B_distrib, x = unit(0.5, "npc"), y = unit(0.5, "npc"), width = 1, height = 1,)
```

---

name: whereML1d

# IV. Where can ML be Put to Use?

```{r, fig.width=9, fig.height=3, echo=FALSE}
library(png)
library(grid)
B_val <- readPNG("pics/B2015_val.png")
grid.raster(B_val, x = unit(0.5, "npc"), y = unit(0.5, "npc"), width = 1, height = 1,)
```

.pull-left[
  
**A**: Predicted wealth composite score
]

.pull-right[

**B**: Actual wealth (Admin 2010, N = 12,792 HHs)
]


**Prediction accuracy for asset ownership was 64 to 92 percent**!

---

name: whereML2

# IV. Where can ML be Put to Use?

## B) Targeting


---

name: whereML3

# IV. Where can ML be Put to Use?

## C) Causality

### Covariate Selection through Regularization (Belloni et al., ReStud 2014)

Paper: “_Inference on Treatment Effects after Selection among High-Dimensional Controls_”

* Scenario: Experimental study, high-dimensional data with $p>n$
* Goal: Select set of predictors $p_{small}<n$

“**Double-selection procedure**”:
1. Use LASSO to select set of predictors for ..
  + .. outcome $y_{i}$
  + .. treatment assignment $d_{i}$
1. Choose predictors in the union of sets as control variables

* Similar literature: 
  + Instrumental variables estimation (e.g., Spiess, 2017; Hartford et al., 2017)
  

---

name: whereML4

# IV. Where can ML be Put to Use?

## D) Hetereogeneity

### Predicting Treatment Heterogeneity

Papers:
  + Athey and Imbens(PNAS 2016) "_Recursive Partitioning for Heterogeneous Causal Effects_"
  + Wagner and Athey (JASA 2017) "_Estimation and Inference of Heterogeneous Treatment Effects using Random Forests_"
  + Davis and Heller (AER 2017) "_Using Causal Forests to Predict Treatment Heterogeneity_"
  
* Random trees and random forests to select covariates to use for heterogeneous treatment estimation


---

name: whereML5

# IV. Where can ML be Put to Use?

## E) Pre-analysis Plans

### Analytical Choices Guided by ML (Ludwig et al., AEAPP 2019)

Paper: "Augmenting Pre-analysis Plans with Machine Learning"

* **Specificity conundrum**: Theory often speaks little to specific choices which PAPs require to prevent data mining (e.g. data aggregation, sub-groups, controls, fct forms)

* **Standard PAP**: Fully specifies entire set of analytical choices
* **Pure ML PAP**:  Start with set of variables and make choices according to prediction problem
* **Augmented PAP**: Take the best of both worlds

Advantages:
1. Benefits from ML flexibility with, at most, limited costs in power
1. Limits the need of researchers to make arbitrary analytical choices
1. Integrates ex-post analysis without risking p-hacking


---

name: caveats1

# V. What Caveats Apply?

## A) Ethics

###


---

name: caveats2

# V. What Caveats Apply?

## B) Opacity or explainability problem ("black box")

```{r, fig.width= 9, fig.height= 5, echo=FALSE}
library(png)
library(grid)
blackbox <- readPNG("pics/Sci_blackbox.png")
grid.raster(blackbox, x = unit(0.5, "npc"), y = unit(0.5, "npc"),width = 1, height = 1,)
```

Link: [Hutson (2018a)](https://www.sciencemag.org/news/2018/05/ai-researchers-allege-machine-learning-alchemy)


---

name: caveats2a

# V. What Caveats Apply?

### Two sides to this argument

Especially for more complex ML algorithms:
 
1. **Opaque process**
  + De-bugging difficult
    
1. **Prediction without regard of DGP**
  + Intelligibility?
  + External validity?
  + Fairness?
  + Trust?
    
    
* Literature: e.g., Ribeiro et al., 2016


---

name: caveats3

# V. What Caveats Apply?

## C) Model Complexity

### Danger of Overfitting

Model $\eta$ explains variation that is unique to training sample
  + Lowers external validity
 
$\rightarrow$ Bias-variance tradeoff

### Many Researcher Degrees of Freedom

* Data pre-processing
  + Example: Text analysis (tokens, n-grams, libraries, etc.)
* ML algorithm
* Regularization parameters
* Cross-validation methods/parameters
* Etc.


---

name: caveats4

# V. What Caveats Apply?

## D) Reproducibility

```{r, fig.width=9, fig.height=4.5, echo=FALSE}
library(png)
library(grid)
repro <- readPNG("pics/Sci_Reproducibility.png")
grid.raster(repro, x = unit(0.5, "npc"), y = unit(0.5, "npc"),width = 1, height = 1,)
```

Link: [Hutson (2018b)](https://science.sciencemag.org/content/359/6377/725)


---

name: caveats4a

# V. What Caveats Apply?

Model complexity has obvious implication for research transparency (and, by extension, reproducibility)
  + ML algorithms may lend themselves to PAPs (see, [Augmented PAPs](#whereML5)) ..
  + .. but **actual practice is a complicated social equilibrium** (incentivization, social norms, tipping points, etc.) 

Potential solution: **Graphical representations**, such as DAGs (Pearl, 1995)
  + Encosing causal assumptions
  + Testability detection
    
- Literature: e.g. Dacrema et al., 2019; Dodge et al., 2019; McDermott et al., 2019; Mitchell et al., 2019.


---

name: caveats5

# V. What Caveats Apply?

## E) Model Robustness or Adaptability

### Problem
    
External validity and out-of-sample prediction
1. Performance under changed environmental conditions]
2. Adversarial attacks

### Solutions
    
* Life-long maschine learning (Chen and Liu, 2016)
* Transfer learning
* Domain adaptation


---

name: caveats6

# V. What Caveats Apply

## F) Causality

### Selection-on-observables assumption 
with optimized set of control variables

### Selective Labels for the Tested


---

name: conclusion

# Conclusion


---

name: extras1

# Additional Slides

## Minimizing Statistical Loss

Minimize empirical risk over space of functions (e.g., linear function, quadratic loss, neural networks, etc.):

\begin{equation}
f ^ {*} = \arg \min_{f \in \mathcal {H}} \frac {1} {n} \sum L \left( f \left( x_{i} \right) , y_{i} \right)
\label{minloss}
\end{equation}

Practical steps:
1. Choose type of loss function, e.g. quadratic $\left( f \left( x_{i} \right) - y_{i} \right) ^ {2}$
1. Choose starting point
1. Use Stochastic Gradient Descent to find minimum in iterative process

```{r, echo=FALSE}
library(png)
library(grid)
SGD_2 <- readPNG("pics/SGD_2.png")
grid.raster(SGD_2, x = unit(0.5, "npc"), y = unit(0.5, "npc"),width = 1, height = 1,)
```


---

name: extras2

# Additional Slides

## $k$-fold cross-validation

<br/>

1. Split data into $k$ sub-samples ("folds")
1. Repeatedly fit model $\eta$ on all but current fold
1. Obtain fitted values of $y_{i}$ for current fold
1. Iteratively cycle through all folds to obtain outcome index of fitted values of $y_{i}$ for all observations