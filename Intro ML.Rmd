---
title: "A Conceptual Introduction to ML"
subtitle: "Internal Presentation, CEGA"
author: "Julius Ruschenpohler"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: inline
---
class: inverse, middle

```{R, setup, include = F}
# devtools::install_github("dill/emoGG")
library(pacman)
p_load(
  broom, tidyverse,
  ggplot2, ggthemes, ggforce, ggridges,
  latex2exp, viridis, extrafont, gridExtra,
  kableExtra, snakecase, janitor,
  data.table, dplyr, estimatr,
  lubridate, knitr, parallel,
  lfe, here, magrittr
)
# Define pink color
red_pink <- "#e64173"
turquoise <- "#20B2AA"
orange <- "#FFA500"
red <- "#fb6107"
blue <- "#3b3b9a"
green <- "#8bb174"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
purple <- "#6A5ACD"
slate <- "#314f4f"
# Dark slate grey: #314f4f
# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  warning = F,
  message = F
)
opts_chunk$set(dev = "svg")
options(device = function(file, width, height) {
  svg(tempfile(), width = width, height = height)
})
options(crayon.enabled = F)
options(knitr.table.format = "html")
# A blank theme for ggplot
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -0.5, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_simple <- theme_bw() + theme(
  line = element_blank(),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text.x = element_text(size = 18, family = "STIXGeneral"),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  # plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_math <- theme_void() + theme(
  text = element_text(family = "MathJax_Math"),
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_serif <- theme_void() + theme(
  text = element_text(family = "MathJax_Main"),
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes <- theme_void() + theme(
  text = element_text(family = "Fira Sans Book"),
  axis.title = element_text(size = 18),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = grey_light,
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_set(theme_gray(base_size = 20))
# Column names for regression results
reg_columns <- c("Term", "Est.", "S.E.", "t stat.", "p-Value")
# Function for formatting p values
format_pvi <- function(pv) {
  return(ifelse(
    pv < 0.0001,
    "<0.0001",
    round(pv, 4) %>% format(scientific = F)
  ))
}
format_pv <- function(pvs) lapply(X = pvs, FUN = format_pvi) %>% unlist()
# Tidy regression results table
tidy_table <- function(x, terms, highlight_row = 1, highlight_color = "black", highlight_bold = T, digits = c(NA, 3, 3, 2, 5), title = NULL) {
  x %>%
    tidy() %>%
    select(1:5) %>%
    mutate(
      term = terms,
      p.value = p.value %>% format_pv()
    ) %>%
    kable(
      col.names = reg_columns,
      escape = F,
      digits = digits,
      caption = title
    ) %>%
    kable_styling(font_size = 20) %>%
    row_spec(1:nrow(tidy(x)), background = "white") %>%
    row_spec(highlight_row, bold = highlight_bold, color = highlight_color)
}
```


.pull-left[
.smaller[

### FIRST PART
I. Standard Regression Inference  
II. Why Machine learning?  
III. What is the Intuition behind ML?
 
]
]

.pull-right[
.smaller[
  
### SECOND PART
IV. Where Can ML be Put to Use?  
V. What Caveats Apply?  
VI. What Are Current Areas of Development?

]
]

---

layout: false
# FIRST PART

## Machine Learning: Why and How?


.pull-left[

### I. [Standard Regression Inference](#standard)
  - [An Inference Problem](#standard)
  - [Link Function](#standard1)
  - [Loss Function](#standard2)
  - [Minimizing Statistical Loss](#standard3)
  
]

.pull-right[

### II. [Why Machine Learning](#whyMLA)
  - [High Dimensionality](#whyMLA)
  - [Prediction vs. Causal Inference](#whyMLB)
  - [Bias-variance Trade-off](#whyMLC)
  
### III. [What is the Intuition behind ML?](#whatMLA)
  - [Goal of Machine Learning](#whatMLA)
  - [Supervised vs. Unsupervised Learning](#whatMLB)
  - [Regularization](#whatMLC)
  - [Classification](#whatMLD)

]


---
name: standard

# I. Standard Regression Inference

## An Inference Problem

Given a limited set of predictors, we would like to predict an outcome: $\mathbb {E} \left[ y_{i} | x_{i} \right]$.

That is, what is $y_{i}$ given $x_{i}$?

- Example: What impact does red meat $x_{i}$ have on general health $y_{i}$?

### Practically, we have to:

.pull-left[

1. Link predictors to outcome (link fct)
1. Define best fit criteria (loss fct)
1. Maximize fit
]

.pull-right[

**Can we make <br/> sense of this <br/> graphically?**
]

---
name: standardA_1

# I. Standard Regression Inference

### Graphical Illustration

.pull-left[

Data on daily red meat consumption

$$X = \begin{bmatrix} 
    x_{1,1} & x_{1,2} & \dots \\
    \vdots & \ddots & \\
    x_{n,1} &        & x_{n,p} 
    \end{bmatrix}$$
]

.pull-right[

Data on daily measures of blood pressure

$$Y = \begin{bmatrix} 
    y_{1,1} & y_{1,2} & \dots \\
    \vdots & \ddots & \\
    y_{n,1} &        & y_{n,k} 
    \end{bmatrix}$$
]

**Link fct**: What is the relationship that links meat consumption and blood pressure?

```{r, echo=FALSE}
library(png)
library(grid)
fctForms <- readPNG("pics/fctForms.png")
grid.raster(fctForms, x = unit(0.5, "npc"), y = unit(0.75, "npc"),width = 0.84, height = 0.4,)
```

**Loss fct**:




---
name: standard1

# I. Standard Regression Inference

## 1. Link Function

First, we need to link the data on meat consumption with the data on blood pressure:

$$\mathbb { E } \left[ y_{i} | x_{i} \right] = f \left( \eta _ { i } \right)$$

Here, $\eta_{i}$ is our model to predict blood pressure.

For simplicity, let blood pressure be a linear fct of meat consumption:

$$\eta _ { i } = \alpha + x_{i} \beta$$

That is, we assume that a straight line is a good approximation for the impact of meat consumption on blood pressure.

But we still don't know **_what_ straight line best approximates the relationship**!

---
name: standard2

# I. Standard Regression Inference

## 2. Loss Function

Second, we need to define "best fit".

Equivalent to saying we need to minimize statistical loss $\ell ( \alpha , \beta )$

Let the loss fct $\ell(\alpha,\beta)$ be quadratic (as per "ordinary least squares"):

$$\ell(\alpha,\beta) = \sum_{i=1} ^ {n} \left( y_{i} - \eta_{i} \right)^{2}$$

That is, we consider the squared distance between our model prediction $\eta_{i}$ of blood pressure and real measures of blood pressure $y_{i}$.

Now, **how to apply this criterion**?

---
name: standard3

# I. Standard Regression Inference

## 3. Minimizing Statistical Loss (~Risk)

Now, let's find the point at which $\ell(\alpha,\beta)$ is smallest. That is, $\min \{\ell(\alpha,\beta) \}$.

### Practically, we have to:

1. Choose a starting point
1. Use Stochastic Gradient Descent to find minimum in iterative process

```{r, echo=FALSE}
library(png)
library(grid)
SGD_2 <- readPNG("pics/SGD_2.png")
grid.raster(SGD_2, x = unit(0.5, "npc"), y = unit(0.75, "npc"),width = 0.5, height = 0.5,)
```




---
name: whyMLB

# II. Why Machine Learning?

## A) High Dimensionality and Sparcity

But what if we are not only interested in the effect of meat consumption?

What if the set of predictors is **huge**?




---
name: whyMLC

# II. Why Machine Learning?

## B) Prediction =/= Causal Inference
Collinearity


---
name: whyMLD

# II. Why Machine Learning?

## D) Bias-variance Trade-off


---
name: whatMLA

# III. What is the Intuition behind ML?

## A) Goal of Machine Learning

### Variable Shrinkage and Selection

Construct a function mapping features (~covariates) onto labels (~outcomes) that generalizes to new data


### Dimensionality Reduction

Identify optimal base vector to most efficiently describe data


---
name: whatMLB

# III. What is the Intuition behind ML?

## B) Supervised vs. Unsupervised ML

### Supervised Machine Learning

Predicting a label (~outcome)
+ Minimizing some kind of prediction error

### Unsupervised Machine Learning

Classify data 
+ Minimizing misclassification


- This presentation focusses on supervised ML.


---
name: whatMLC

# III. What is the Intuition behind ML?





---
name: whatMLD

# III. What is the Intuition behind ML?

## D) Penalized Regression

Open question: **How to deal with high dimensionality in data?**

Let's remember the loss fct: $\ell(\alpha,\beta)$

- However, now the set of potential predictors is too large!

To make the model less complex, we would like to select only the most important predictors.

Let's penalize model complexity:

$$\min \left\{\ell(\alpha,\beta) + n \lambda \sum_{j=1}^{p} K_{j} \left (\beta_{j} \right) \right\}$$

Shrinkage: $\lambda$ governs the magnitude of penalty or shrinkage.

Regularization: $K_{j} \left( \beta_{j} \right)$ is a cost fct that defines the penalty regime


---
name: whatMLD1

# III. What is the Intuition behind ML?

## D) Penalized Regression

### Types of cost fcts

1. Ridge
1. Lasso
1. Elastic net
1. Log

But how much do we want to penalize complexity? 

That is, **how to choose $\lambda$**?

We need to validate a chosen $\lambda$!


---
name: whatMLD2

# III. What is the Intuition behind ML?

### 1. Training and Test Sample

.pull-left[

$$X_{train} = \begin{bmatrix} 
    x_{1,1} & x_{1,2} & \dots \\
    \vdots & \ddots & \\
    x_{n,1} &        & x_{n,p} 
    \end{bmatrix}$$
    
$$X_{test} = \begin{bmatrix} 
    x_{1,1} & x_{1,2} & \dots \\
    \vdots & \ddots & \\
    x_{n,1} &        & x_{n,p} 
    \end{bmatrix}$$
]


.pull-right[

$$Y_{train} = \begin{bmatrix} 
    y_{1,1} & y_{1,2} & \dots \\
    \vdots & \ddots & \\
    y_{n,1} &        & y_{n,k} 
    \end{bmatrix}$$
    
$$Y_{test} = \begin{bmatrix} 
    y_{1,1} & y_{1,2} & \dots \\
    \vdots & \ddots & \\
    y_{n,1} &        & y_{n,k} 
    \end{bmatrix}$$
]

### 2. K-fold cross-validation

### 3. Leave-one-out cross-validation


---
name: whatMLE

# III. What is the Intuition behind ML?

## E) Classification/Non-parametric Methods




---

layout: false
# SECOND PART

## Application, Caveats, and Developments in ML


.pull-left[

### IV. [Where to Use ML?](#whereML1)
  - [Prediction](#whereML1)
  - [Targeting](#whereML2)
  - [Measurement](#whereML3)
  - [Heterogeneity](#whereML4)
  - [Pre-analysis Plans](#whereML5)
  
### V. [What Caveats Apply](#caveats1)
  - [Ethics](#caveats1)
  - [Opacity](#caveats2)
  - [Model Complexity](#caveats3)
  - [Reproducibility](#caveats4)
  - [Model Robustness](#caveats5)
  - [Causality](#caveats6)
  
]

.pull-right[
  
### VI. [Current Areas of Development](#develop1)
  - [Prediction](#develop1)
  - [Model Robustness](#develop2)
  - [Causality and heterogeneity](#develop3)

]



---

name: whereML1

# IV. Where can ML be Put to Use?

## A) Prediction and Measurement

### Predicting Firm Performance from Surveys


---

name: whereML1a

# IV. Where can ML be Put to Use?

### Predicting Poverty from Satellite Images


---

name: whereML2

# IV. Where can ML be Put to Use?

## B) Targeting


---

name: whereML3

# IV. Where can ML be Put to Use?

## C) Causality

### Covariate selection through Regularization

### Boosting Relevance of Instrumental Variables


---

name: whereML4

# IV. Where can ML be Put to Use?

## D) Hetereogeneity

### Optimal Selection of Heterogeneous Variables


---

name: whereML5

# IV. Where can ML be Put to Use?

## E) Pre-analysis Plans

### Decision Heuristics Based on ML


---

name: caveats1

# V. What Caveats Apply?

## A) Ethics

###


---

name: caveats2

# V. What Caveats Apply?

## B) Opacity or explainability ("black box")

```{r, echo=FALSE}
library(png)
library(grid)
blackbox <- readPNG("pics/Sci_blackbox.png")
grid.raster(blackbox, x = unit(0.4, "npc"), y = unit(0.6, "npc"),width = 0.75, height = 0.7,)
```

[Hutson, 2018a] (https://www.sciencemag.org/news/2018/05/ai-researchers-allege-machine-learning-alchemy)


---

name: caveats2a

# V. What Caveats Apply?

### Two sides to this argument
   
1. **Opaque process**
+ De-bugging
    
2. **Prediction without regard of DGP**
+ Intelligibility
+ Trust
+ External validity
    
    
- Literature: see, e.g., Ribeiro et al., 2016


---

name: caveats3

# V. What Caveats Apply?

## C) Model Complexity

### Many researcher degrees of freedom

* Data pre-processing
  + Example: Text analysis (libraries, tokenization, n-grams, etc.)

* Shrinkage and Regularization

* Cross-validation



---

name: caveats4

# V. What Caveats Apply?

## D) Reproducibility

```{r, echo=FALSE}
library(png)
library(grid)
repro <- readPNG("pics/Sci_Reproducibility.png")
grid.raster(repro, x = unit(0.4, "npc"), y = unit(0.6, "npc"),width = 0.7, height = 0.45,)
```

    Hutson, 2018b (https://science.sciencemag.org/content/359/6377/725)

    
---

name: caveats4a

# V. What Caveats Apply?

### Encoding causal assumptions
    
* Testability detection via graphical representations (e.g., DAGs a la Pearl, 1995)
    
- LIterature: see, e.g. Dacrema et al., 2019; Dodge et al., 2019; McDermott et al., 2019; Mitchell et al., 2019.


---

name: caveats5

# V. What Caveats Apply?

## E) Model Robustness or Adaptability

### Problem
    
External validity and out-of-sample prediction
1. Performance under changed environmental conditions]
2. Adversarial attacks

### Solutions
    
* Life-long maschine learning (Chen and Liu, 2016)
* Transfer learning
* Domain adaptation


---

name: caveats6

# V. What Caveats Apply

## F) Causality

### Selection-on-observables assumption 
with optimized set of control variables

### Selective Labels for the Tested



---

name: develop1

# VI. Current Areas of Development

## A) Re: Prediction


---

name: develop2

# VI. Current Areas of Development

## A) Re: Model Robustness


---

name: develop3

# VI. Current Areas of Development

## B) Re: Causality and Heterogeneous Treatment Effects




---

name: conclusion

# Conclusion


---

name: extras

# Additional Slides

## Minimizing Statistical Loss

Minimize empirical risk over space of functions (e.g., linear function, quadratic loss, neural networks, etc.):

\begin{equation}
f ^ {*} = \arg \min_{f \in \mathcal {H}} \frac {1} {n} \sum L \left( f \left( x_{i} \right) , y_{i} \right)
\label{minloss}
\end{equation}

Practical steps:
* Choose type of loss function, e.g. quadratic $\left( f \left( x_{i} \right) - y_{i} \right) ^ {2}$
* Choose starting point
* Optimize using Stochastic Gradient Descent


